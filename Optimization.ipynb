{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization\n",
    "\n",
    "As mentioned in the model, this script was generated to find the best parameters of the handwritten digits recognition. Two parameters (also called hyperparameters) optimized were `batch_size` and number of `epochs`. This example can be extrapolated to your hyperparameters as well in order to improve the accuracy of the model. However, given the accuracy is 98% at the moment the optimization was stop here.\n",
    "\n",
    "The first lines of the code as similar to the ones in the model. The difference here is the used of `GridSearchCV` from `scikit-learn` which allows us to change parameters of a particular model until we find the best parameters. This method does not add a bias to the model despite using the training set because it uses cross-validation which is a method of folding the data into segments and test the hyperparameters there. In other words, the optimization of hyperparameters do not use the test set as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train,y_train), (x_test,y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = tf.keras.utils.normalize(x_train, axis=1)\n",
    "x_test = tf.keras.utils.normalize(x_test, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    model.add(tf.keras.layers.Dense(128, activation=tf.nn.relu))\n",
    "    model.add(tf.keras.layers.Dense(128, activation=tf.nn.relu))\n",
    "    model.add(tf.keras.layers.Dense(10, activation=tf.nn.softmax))\n",
    "    model.compile(optimizer='adam',\n",
    "             loss='sparse_categorical_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fix random seed for reproducibility\n",
    "seed = 7\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.wrappers.scikit_learn.KerasClassifier(build_fn=create_model, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can define the different values of the parameters that we want to test. More values could be added with the penalty of a longer computing time. The trick is always balancing the cost-benefit. The different combinations are run and saved under `grid_results`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the grid search parameters\n",
    "batch_size = [10,50,100,200]\n",
    "epochs = [3,10,20,40]\n",
    "param_grid = dict(batch_size = batch_size, epochs = epochs)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\n",
    "grid_result = grid.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the combinations have been performed, we can analyze which combination is the most optimal for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.972283 using {'batch_size': 10, 'epochs': 40}\n",
      "0.965633 (0.002690) with: {'batch_size': 10, 'epochs': 3}\n",
      "0.970267 (0.002123) with: {'batch_size': 10, 'epochs': 10}\n",
      "0.969450 (0.001150) with: {'batch_size': 10, 'epochs': 20}\n",
      "0.972283 (0.003406) with: {'batch_size': 10, 'epochs': 40}\n",
      "0.960300 (0.002526) with: {'batch_size': 50, 'epochs': 3}\n",
      "0.968667 (0.002490) with: {'batch_size': 50, 'epochs': 10}\n",
      "0.970433 (0.001190) with: {'batch_size': 50, 'epochs': 20}\n",
      "0.970100 (0.004351) with: {'batch_size': 50, 'epochs': 40}\n",
      "0.957933 (0.001782) with: {'batch_size': 100, 'epochs': 3}\n",
      "0.968483 (0.001164) with: {'batch_size': 100, 'epochs': 10}\n",
      "0.968450 (0.002266) with: {'batch_size': 100, 'epochs': 20}\n",
      "0.971050 (0.002541) with: {'batch_size': 100, 'epochs': 40}\n",
      "0.950133 (0.001185) with: {'batch_size': 200, 'epochs': 3}\n",
      "0.967417 (0.002374) with: {'batch_size': 200, 'epochs': 10}\n",
      "0.969650 (0.001101) with: {'batch_size': 200, 'epochs': 20}\n",
      "0.970750 (0.001564) with: {'batch_size': 200, 'epochs': 40}\n"
     ]
    }
   ],
   "source": [
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or, we can simply do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': 10, 'epochs': 40}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_result.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice one thing here. The best parameters are `batch_size` equals 10 and 40 `epochs`. However, using the same `batch_size` but only 10 `epochs` keeps the accuracy high while only taking a tench of the time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
